{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzi0erHpGywX"
   },
   "source": [
    "# Hướng dẫn homework 6\n",
    "---\n",
    "- Bạn cần đọc đề bài trước khi đọc hướng dẫn.\n",
    "- Tốt nhất là nên tự làm trước rồi mới xem hướng dẫn, vì như vậy sẽ giúp rèn luyện khả năng giải quyết vấn đề từ điểm xuất phát là 0. Một vấn đề có thể có nhiều cách giải quyết, cách này có thể sẽ hiệu quả hơn cách kia; sau khi tự làm, bạn có thể xem hướng dẫn và so sánh cách làm của bạn với cách làm của hướng dẫn.\n",
    "- Không phải tất cả các câu đều sẽ có hướng dẫn (phần hướng dẫn chủ yếu tập trung vào các câu liên quan đến lập trình).\n",
    "- Nếu vẫn có thắc mắc gì khác thì bạn có thể post lên moodle.\n",
    "- Ở markdown cell của notebook, bạn có thể gõ công thức theo latex bằng cách bọc trong `$...$` (dạng inline), hoặc `$$...$$` (ví dụ, `$\\mu$` sẽ ra $\\mu$, `$\\nu$` sẽ ra $\\nu$, `$\\lambda$` sẽ ra $\\lambda$, `$\\epsilon$` sẽ ra $\\epsilon$, ...).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vbePR01xbBT"
   },
   "source": [
    "## Câu 1: Overfitting and Deterministic Noise\n",
    "\n",
    "Định nghĩa của deterministic noise (slide 16, lecture 11 - overfitting): \"The part of $f$ that $\\mathcal H$ cannot capture: $f(\\mathbf{x}) − h^∗(\\mathbf{x})$\". Trong đó, $h^*(\\mathbf{x})$ là hàm tốt nhất của $\\mathcal H$ dùng để xấp xỉ $f$.\n",
    "\n",
    "\"Deterministic noise\" sẽ thay đổi như thế nào nếu dùng $\\mathcal H' \\subset  \\mathcal H$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cVt_qZ0Gywa"
   },
   "source": [
    "## Câu 2-6: Regularization with Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đọc dữ liệu từ file vào mảng Numpy**\n",
    "\n",
    "Bạn có thể thực hiện việc này một cách nhanh chóng bằng hàm `np.loadtxt`. Ví dụ, để đọc file \"in.dta\" vào mảng \"train_data\":\n",
    "\n",
    "    import numpy as np\n",
    "    train_data = np.loadtxt(\"in.dta\")\n",
    "    \n",
    "Kết quả `train_data` sẽ là một mảng Numpy 2 chiều, có 3 cột, trong đó: cột cuối ứng với các output $y$, các cột còn lại ứng với các véc-tơ input $\\mathbf{x}$. Từ mảng `train_data` này, bạn có thể tách ra mảng `train_X` và `train_Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression cho classification**\n",
    "\n",
    "Như đã học:\n",
    "- Khi huấn luyện, ta sẽ xem output $y=\\pm 1$ là số thực và tìm $w$ một cách nhanh chóng bằng công thức ở slide 11, lecture 12 - regularization. Bạn có thể chỉ cần cài đặt công thức tính $\\mathbf{w_{reg}}$, khi không dùng regularization thì cho $\\lambda=0$. Trong công thức tính $\\mathbf{w_{reg}}$, $\\mathbf{I}$ là ma trận đơn vị (ma trận vuông, các phần tử trên đường chéo chính bằng 1, còn lại là 0). Trong Numpy, để tạo ra một ma trận đơn vị, giả sử có kích thước $5\\times5$, thì bạn có thể dùng câu lệnh: `np.eye(5)`. Trong công thức tính $\\mathbf{w_{reg}}$ thì $\\mathbf{I}$ là ma trận đơn vị có kích thước bằng với kích thước của $\\mathbf{Z}^T\\mathbf{Z}$.\n",
    "- Sau khi đã có $\\mathbf{w}$, để dự đoán lớp ($\\pm1$) của một véc-tơ input $\\mathbf{x}$ thì dùng công thức: $sign(\\mathbf{w}^T\\mathbf{x})$. Để ý $E_{in}$ và $E_{out}$ mà đề bài hỏi là độ lỗi nhị phân khi phân lớp (chứ không phải độ lỗi bình phương khi hồi qui)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ảnh hưởng của siêu tham số $\\lambda$**\n",
    "\n",
    "Như đã học, $\\lambda$ cho biết mức độ kìm hãm khả năng fit của mô hình. $\\lambda$ càng lớn thì mô hình sẽ càng bị kìm hãm khả năng fit, $E_{in}$ nói chung sẽ càng lớn, còn $E_{out}$ thì sao? Nếu khi chưa dùng regularization ($\\lambda=0$) mà mô hình đã bị overfit thì khi tăng $\\lambda$ nói chung sẽ giúp mô hình đỡ fit phải nhiễu và do đó giúp $E_{out}$ giảm xuống; nhưng khi tăng $\\lambda$ quá một mức nào đó thì sẽ làm mô hình không đủ khả năng để fit được cái nên fit và do đó $E_{out}$ lại tăng lên. Còn nếu khi chưa dùng regularization ($\\lambda=0$) mà mô hình không bị overfit thì khi tăng $\\lambda$ lên sẽ không giúp làm giảm $E_{out}$.\n",
    "\n",
    "Ở câu 5 và 6, khi bạn thí nghiệm với $\\lambda$ tăng dần lên thì có thể sẽ không thấy một cách rõ ràng điều mình mô tả ở trên. Lý do là vì: khi học, độ lỗi mà Linear Regression trực tiếp cực tiểu hóa là độ lỗi bình phương nhưng mà độ lỗi in ra xem lại là độ lỗi nhị phân (sau khi lấy sign của $\\mathbf{w}^T\\mathbf{x}$). Bạn thử in ra thêm độ lỗi bình phương (không lấy sign của $\\mathbf{w}^T\\mathbf{x}$) và xem có giống điều mình mô tả ở trên khi tăng $\\lambda$ không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EswJI0uBGywc"
   },
   "source": [
    "## Câu 7: Regularization for Polynomials\n",
    "\n",
    "Câu này mình nghĩ là bạn có thể tự làm được. \n",
    "\n",
    "Mình chỉ muốn nói thêm một ít về phép transform được nói đến trong đề bài:\n",
    "$$x \\in \\mathbb{R} \\rightarrow \\mathbf{z} \\in \\mathbb{R}^{Q+1}$$\n",
    "với $\\mathbf{z}$ gồm các thành phần: $L_0(x)=0, L_1(x), L_2(x), ..., L_Q(x)$. Trong đó $L_q(x)$ là đa thức Legendre bậc $q$; bạn có thể xem một số đa thức Legendre ở slide 6, lecture 12 - regularization. Tập hypothesis của Linear Regression trong không gian $\\mathcal{Z}$ theo phép biến đổi ở trên là tập các đa thức bậc $Q$. \n",
    "\n",
    "Một cách khác là dùng Linear Regression trong không gian $\\mathcal{Z}$ có được bằng phép biến đổi: $x \\rightarrow \\mathbf{z} = [1, x, x^2, ..., x^Q]^T$. 2 tập hypothesis ứng với 2 cách làm này là tương đương nhau và đều là tập đa thức bậc $Q$. \n",
    "\n",
    "Điểm khác nhau mà mình thấy là: ở cách làm thứ 2, các thành phần trong véc-tơ $\\mathbf{z}$ có range khác nhau và dẫn đến các trọng số $w$ tương ứng cũng sẽ có range khác nhau (ví dụ, nếu $x \\in [-1, 1]$ thì $x^q$ sẽ có range càng nhỏ khi $q$ càng lớn, và $w_q$ tương ứng sẽ có range càng lớn); vụ range khác nhau này có thể sẽ gây ra một số vấn đề (chẳng hạn, nếu dùng regularization $\\sum_i w_i^2$ thì việc các $w_i$ có range khác nhau sẽ làm cho một số $w_i$ bị tập trung kìm hãm hơn, đây có thể là điều ta không muốn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_BUaPhoGywe"
   },
   "source": [
    "## Câu 8-10: Neural Networks\n",
    "\n",
    "**Câu 8:**\n",
    "\n",
    "Một lần backpropagation với một mẫu dữ liệu $(\\mathbf{x}, y)$ mà đề bài hỏi gồm các bước:\n",
    "1. Bắt đầu từ véc-tơ input $\\mathbf{x}$, thực hiện lan truyền tiến qua các tầng của mạng để tính ra các giá trị đầu ra $a$ của các nơ-ron. Lưu ý: ở đây mình dùng ký hiệu $a^{(l)}_i$ để chỉ giá trị đầu ra của nơ-ron $i$ ở tầng $l$, còn trong đề bài là dùng ký hiệu $x^{(l)}_i$. Có bao nhiêu phép tính $w^{(l)}_{ij}a^{(l-1)}_i$ ở bước lan truyền tiến này? (lưu ý: $d^{(l)}$ là số lượng nơ-ron ở tầng $l$ mà chưa tính nơ-ron luôn có giá trị $+1$.)\n",
    "2. Từ $\\delta$ ở tầng cuối, thực hiện lan truyền ngược để tính $\\delta$ của các nơ-ron ở các tầng trước đó. Có bao nhiêu phép tính $w^{(l)}_{ij}\\delta^{(l)}_j$ ở bước này? Gợi ý: mục tiêu của việc tính các $\\delta$ là để tính các đạo hàm riêng (ở bước 3), sẽ có một số nơ-ron sẽ không cần tính $\\delta$ vì nếu tính thì không dùng để làm gì cả.\n",
    "3. Với mỗi trọng số $w^{(l)}_{ij}$, tính $\\frac{\\partial e}{\\partial w^{(l)}_{ij}}$ bằng cách lấy $a$ ở một đầu (nơ-ron $i$ ở tầng $l-1$) nhân với $\\delta$ ở đầu kia (nơ-ron $j$ ở tầng $l$). Có bao nhiêu phép tính $a^{(l-1)}_i\\delta^{(l)}_j$ ở bước này?\n",
    "\n",
    "<font color=green>\"Correct output\" của mình: 47.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_BUaPhoGywe"
   },
   "source": [
    "**Câu 9, 10:**\n",
    "\n",
    "Bạn có thể lập luận hoặc viết một chương trình xét tất cả các trường hợp. Lưu ý: ở mỗi tầng ẩn phải có một nơ-ron \"+1\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5-Guide.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
