{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huỳnh Minh Huấn - 1612858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 1: Linear Regression Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.99999999999996"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Có thể sử dụng hàm find_N() => tìm ra được lowerBound của N thỏa câu 1\n",
    "def find_N(sigma, d, val):\n",
    "    return (d + 1)/(1 - 0.008 / math.pow(sigma, 2))\n",
    "find_N(0.1, 8, 0.008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_Ein(N, sigma, d):\n",
    "    return math.pow(sigma, 2) * (1-(d + 1)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 10 => Ed(Ein) = 0.001\n",
      "N = 25 => Ed(Ein) = 0.006400000000000001\n",
      "N = 100 => Ed(Ein) = 0.009100000000000002\n",
      "N = 500 => Ed(Ein) = 0.009820000000000002\n",
      "N = 1000 => Ed(Ein) = 0.009910000000000002\n"
     ]
    }
   ],
   "source": [
    "sigma = 0.1\n",
    "d = 8\n",
    "print(\"N = 10 => Ed(Ein) =\", E_Ein(10, sigma, d))\n",
    "print(\"N = 25 => Ed(Ein) =\", E_Ein(25, sigma, d))\n",
    "print(\"N = 100 => Ed(Ein) =\", E_Ein(100, sigma, d))\n",
    "print(\"N = 500 => Ed(Ein) =\", E_Ein(500, sigma, d))\n",
    "print(\"N = 1000 => Ed(Ein) =\", E_Ein(1000, sigma, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Ta thấy với N = 100 (giá trị nhỏ nhất), thì kỳ vọng $E_{in}\\ge0.008$\n",
    "\n",
    "**Chọn [c] 100.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 2 - 3: Nonlinear Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Ta có: $h(\\mathbf{x}) = sign(\\tilde{w}_0 + \\tilde{w}_1 x_1^2 + \\tilde{w}_2 x_2^2)$\n",
    "\n",
    "Khi $x_1$ siêu âm hoặc siêu dương thì $x_1^2$ sẽ dương và rất lớn, và  $\\tilde{w}_1 x_1^2$ sẽ là đại lượng quyết định dấu của tổng trong công thức ở trên. Do $x_1^2$ dương nên $\\tilde{w}_1$ sẽ quyết định dấu của $\\tilde{w}_1 x_1^2$. Ta thấy dấu của tổng trong công thức trên sẽ âm khi |x1| lớn (x1 siêu âm hoặc siêu dương) - nhìn vào đồ thị. Do đó $\\tilde{w}_1 < 0$.\n",
    "\n",
    "Tương tự, khi $x_2$ siêu âm hoặc siêu dương thì $x_2^2$ sẽ dương và rất lớn, và  $\\tilde{w}_2 x_2^2$ sẽ là đại lượng quyết định dấu của tổng trong công thức ở trên. Do $x_2^2$ dương nên $\\tilde{w}_2$ sẽ quyết định dấu của $\\tilde{w}_2 x_2^2$. Ta thấy dấu của tổng trong công thức trên sẽ dương khi |x2| lớn (x2 siêu âm hoặc siêu dương) - nhìn vào đồ thị. Do đó $\\tilde{w}_2 > 0$.\n",
    "\n",
    "Vậy: $\\tilde{w}_1 < 0,\\tilde{w}_2 > 0$.\n",
    "\n",
    "**Chọn [d] $\\tilde{w}_1 < 0,\\tilde{w}_2 > 0$.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Sử dụng công thức về $d_{VC}$ của mô hình phân lớp tuyến tính (Perceptron) trong không gian $\\mathcal{Z}$ ở slide 4, lecture 9 - the linear model II ${d_{VC}\\leq d^*+ 1}$ với $d^*+ 1$ là số chiều của w* trong không gian $\\mathcal{Z}$ (không gian khi chuyển áp dụng non-linear transform).\n",
    "\n",
    "Ta có: d* + 1 = 15, suy ra $d_{VC} \\leq 15$.\n",
    "\n",
    "Do đó: giá trị nhỏ nhất trong các đáp án mà không nhỏ hơn $d_{VC}$ là: 15.\n",
    "\n",
    "**Chọn [c] 15.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 4 - 7: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "${\\frac{\\partial E}{\\partial u} = \\frac{\\partial(ue^v-2ve^{-u})^2}{\\partial u}}$\n",
    "\n",
    "${\\frac{\\partial E}{\\partial u} = 2(ue^v-2ve^{-u})(ue^v-2ve^{-u})'}$\n",
    "\n",
    "**Kí hiệu:** $(ue^v-2ve^{-u})' = \\frac{\\partial(ue^v-2ve^{-u})}{\\partial u}$ do đang xét đạo hàm riêng theo u nên em ghi vậy cho gọn.\n",
    "\n",
    "Ta có:\n",
    "\n",
    "- $\\frac{\\partial(ue^v)}{\\partial u} = e^v$\n",
    "- $\\frac{\\partial(-2ve^{-u})}{\\partial u} = -2*(-u)'ve^{-u} = 2ve^{-u}$\n",
    "\n",
    "Do đó: ${\\frac{\\partial E}{\\partial u} = 2(ue^v-2ve^{-u})(e^v + 2ve^{-u})}$\n",
    "\n",
    "**Chọn [e] ${2(e^v + 2ve^{-u})(ue^v-2ve^{-u})}$.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hàm Gradient tính gradient của E và Hàm Gradient_descent chạy thuật toán GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(point):\n",
    "    \"\"\"\n",
    "    input: point(u, v)\n",
    "    output: [du, dv]\n",
    "    \"\"\"\n",
    "    du = 2 * (np.exp(point[1]) + 2*point[1]*np.exp(-point[0]))*(point[0]*np.exp(point[1])-2*point[1]*np.exp(-point[0]))\n",
    "    dv = 2 * (point[0]*np.exp(point[1])-2*point[1]*np.exp(-point[0])) * (point[0]*np.exp(point[1])-2*np.exp(-point[0]))\n",
    "    return np.array([du, dv])\n",
    "\n",
    "def Gradient_Descent(eta, threshold):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        eta - leanring rate\n",
    "        threshold - ngưỡng\n",
    "    ouput:\n",
    "        iterations - số lần lặp\n",
    "        point - điểm cực tiểu cục bộ tìm được\n",
    "    \"\"\"\n",
    "    point = np.array([1, 1])\n",
    "    Error = math.pow(1*np.exp(1) - 2*1*np.exp(-1), 2)\n",
    "    iterations = 0\n",
    "    while Error >= threshold:\n",
    "        iterations += 1\n",
    "        point = point - eta * Gradient(point)\n",
    "        Error = math.pow(point[0]*np.exp(point[1]) - 2*point[1]*np.exp(-point[0]), 2)\n",
    "    return iterations, point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations = 10\n",
      "Point = [0.04473629 0.02395871]\n"
     ]
    }
   ],
   "source": [
    "def main_5_6(eta, threshold):\n",
    "    #eta = 0.1\n",
    "    #threshold = 1e-14\n",
    "    iterations, point = Gradient_Descent(eta, threshold)\n",
    "    print(\"Iterations =\",iterations)\n",
    "    print(\"Point =\",point)\n",
    "main_5_6(eta = 0.1, threshold = 1e-14)\n",
    "#print(Gradient([13.69542993, 7.86079446]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Chạy hàm main_5_6(eta = 0.1, threshold = 1e-14), ta được kết quả: **Iterations = 10**.\n",
    "\n",
    "**Chọn [d] 10.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Chạy hàm main_5_6(eta = 0.1, threshold = 1e-14), ta được kết quả: **Point = [0.04473629 0.02395871]**. Gần nhất với đáp án **(0.045, 0.024)**.\n",
    "\n",
    "**Chọn [e] (0.045,0.024).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.13981379199615315\n"
     ]
    }
   ],
   "source": [
    "def main_7(eta, N):\n",
    "    #eta = 0.1\n",
    "    #N = 15\n",
    "    grad = np.array([1, 1])\n",
    "    for i in range(N):\n",
    "        du = 2 * (np.exp(grad[1]) + 2*grad[1]*np.exp(-grad[0]))*(grad[0]*np.exp(grad[1])-2*grad[1]*np.exp(-grad[0]))\n",
    "        grad = grad - eta * np.array([du, 0])\n",
    "        dv = 2 * (grad[0]*np.exp(grad[1])-2*grad[1]*np.exp(-grad[0])) * (grad[0]*np.exp(grad[1])-2*np.exp(-grad[0]))\n",
    "        grad = grad - eta * np.array([0, dv])\n",
    "    Error = math.pow(grad[0]*np.exp(grad[1]) - 2*grad[1]*np.exp(-grad[0]), 2)\n",
    "    print(\"Error = \", Error)\n",
    "main_7(eta = 0.1, N = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "Chạy hàm main_7(eta = 0.1, N = 15), ta được kết quả: **Error =  0.13981379199615315**. Gần nhất với đáp án **0.1**.\n",
    "\n",
    "**Chọn [a] $10^{-1}$.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 8 - 9: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phát sinh ra `target_w`, véc-tơ tham số của $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_w():\n",
    "    \"\"\"\n",
    "    Generates target_w from two random, uniformly distributed points in [-1, 1] x [-1, 1].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \"\"\"\n",
    "    # Generate two points from a uniform distribution over [-1, 1]x[-1, 1]\n",
    "    p1 = np.random.uniform(-1, 1, 2)\n",
    "    p2 = np.random.uniform(-1, 1, 2)\n",
    "    # Compute the target W from these two points\n",
    "    target_w = np.array([p1[1]*p2[0] - p1[0]*p2[1], p2[1] - p1[1], p1[0] - p2[0]]).reshape((-1, 1))\n",
    "    \n",
    "    return target_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phát sinh ra tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, target_w):\n",
    "    \"\"\"\n",
    "    Generates a data set by generating random inputs and then using target_w to generate the \n",
    "    corresponding outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of examples.\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.        \n",
    "    \"\"\"\n",
    "    bad_data = True # `bad_data = True` means: data contain points on the target line \n",
    "                    # (this rarely happens, but just to be careful)\n",
    "                    # -> y's of these points = 0 (with np.sign); \n",
    "                    #    we don't want this (y's of data must be -1 or 1)\n",
    "                    # -> re-generate data until `bad_data = False`\n",
    "    \n",
    "    while bad_data == True:\n",
    "        X = np.random.uniform(-1, 1, (N, 2))\n",
    "        X = np.hstack((np.ones((N, 1)), X)) # Add 'ones' column\n",
    "        Y = np.sign(np.dot(X, target_w))\n",
    "        if (0 not in Y): # Good data\n",
    "            bad_data = False\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chạy thuật toán Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, Y, lr = 0.01):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        X : numpy array, shape (N, 3)\n",
    "        Y : numpy array, shape (N, 1)    \n",
    "        lr: float\n",
    "    output:\n",
    "        w : numpy array, shape(3)\n",
    "        n_epochs: int\n",
    "    \"\"\"\n",
    "    N, sizeW = X.shape\n",
    "    w = np.zeros([sizeW])\n",
    "    #print(w)\n",
    "    n_epochs = 0\n",
    "    epochs = []\n",
    "    E_ins = []\n",
    "    while True:\n",
    "        pre_w = np.copy(w)\n",
    "        rand_idx = np.random.permutation(N)\n",
    "        for id in rand_idx:\n",
    "            # do something\n",
    "            gradient = (-Y[id] * X[id,:])/(1 + np.exp(Y[id]*np.dot(w.T, X[id,:])))\n",
    "            w = w - lr * gradient \n",
    "        #E_in = 0\n",
    "        #for j in range(100):\n",
    "        #    E_in += np.log(1 + np.exp(-Y[j]*np.dot(w.T, X[j])))\n",
    "        n_epochs += 1\n",
    "        #epochs.append(n_epochs)\n",
    "        #E_ins.append(E_in)\n",
    "        if np.linalg.norm(pre_w - w) < 0.01:\n",
    "            break\n",
    "    #plt.plot(epochs, E_ins)\n",
    "    #plt.show()\n",
    "    return w.reshape(3), n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87429193 8.28347753 4.20687108]\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "target_w = generate_target_w()\n",
    "X, Y = generate_data(100, target_w)\n",
    "w, n_epochs = SGD(X, Y, lr = 0.01)\n",
    "print(w)\n",
    "print(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_Eout =  0.10189830919779982\n",
      "average_epochs =  340.11\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "def main_8_9():\n",
    "    N = 100 # data training points\n",
    "    epochs = []\n",
    "    Eouts = []\n",
    "    average_Eout = 0\n",
    "    # Repeat the experiment for 100 runs with diﬀerent targets and take the average.\n",
    "    for i in range(100): \n",
    "        # print(i)\n",
    "        target_w = generate_target_w()\n",
    "        X, Y = generate_data(N, target_w)\n",
    "        w, n_epochs = SGD(X, Y, lr = 0.01)\n",
    "        #Eout = 0\n",
    "        X_new, Y_new = generate_data(N, target_w) # generate new data kích thước N = 1000 để tính Eout\n",
    "        # Tính Eout theo công thức cross entropy với new Data\n",
    "        #for j in range(1000):\n",
    "        #    Eout += np.log(1 + np.exp(-Y_new[j]*np.dot(w.T, X_new[j])))\n",
    "        #Eouts.append(Eout / 1000)\n",
    "        #Eouts.append(np.mean(np.log(1 + np.exp(-Y_new*np.dot(X_new, w)[:, None]))))\n",
    "        average_Eout += np.mean(np.log(1 + np.exp(-Y_new*np.dot(X_new, w)[:, None]))) / N\n",
    "        epochs.append(n_epochs)\n",
    "    print(\"average_Eout = \",np.mean(average_Eout))\n",
    "    print(\"average_epochs = \",np.mean(epochs))\n",
    "%time main_8_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "Chạy hàm main_8_9(), ta được kết quả average_Eout xấp xỉ **0.101...** gần nhất với 0.100.\n",
    "\n",
    "**Chọn [d] 0.100.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "Chạy hàm main_8_9(), ta được kết quả average_epochs xấp xỉ gần nhất 350.\n",
    "\n",
    "**Chọn [a] 350.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 10: PLA vs SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.\n",
    "- PLA: \n",
    "    - Với mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ được phân lớp đúng thì $\\mathbf{w}$ không thay đổi.\n",
    "    - Với mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ bị phân lớp sai thì cập nhật: $\\mathbf{w} \\leftarrow \\mathbf{w} + y^{(n)}\\mathbf{x}^{(n)}$.\n",
    "\n",
    "- SGD: với mỗi mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ thì đều cập nhật $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\times \\mathbf{\\nabla_wE_{in}}$.\n",
    "\n",
    "Xét $\\alpha=1$:\n",
    "SGD mỗi mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ cập nhật: $\\mathbf{w} \\leftarrow \\mathbf{w} -\\mathbf{\\nabla_wE_{in}}$.\n",
    "\n",
    "Ta có $E_{in}=\\frac{1}{N}\\sum_{n=1}^Ne_n$. Suy ra: $\\nabla_wE_{in} = \\nabla_w\\frac{1}{N}\\sum_{n=1}^Ne_n = \\frac{1}{N}\\sum_{n=1}^N\\nabla_w e_n$.\n",
    "\n",
    "Xét: $e_n = -min(0, y^{(n)}w^Tx^{(n)})$ \n",
    "\n",
    "- Với mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ được phân lớp đúng $e_n = 0$ thì $\\nabla_w e_n = 0$. Suy ra: $\\nabla_wE_{in} = 0$, $\\mathbf{w} \\leftarrow \\mathbf{w} - 0 = \\mathbf{w}$\n",
    "\n",
    "- Với mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ bị phân lớp sai $e_n = -y^{(n)}w^Tx^{(n)}$ thì $\\nabla_w e_n = -y^{(n)}x^{(n)}$. Suy ra: $\\nabla_wE_{in} = \\frac{1}{N}-Ny^{(n)}x^{(n)} = -y^{(n)}x^{(n)}$, $\\mathbf{w} \\leftarrow \\mathbf{w} -  (-y^{(n)}x^{(n)}) = \\mathbf{w} + y^{(n)}x^{(n)}$\n",
    "\n",
    "Thỏa PLA nên chọn [e]\n",
    "\n",
    "**Chọn [e] $e_n(w)=-min(0,y_nw^Tx_n)$.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
